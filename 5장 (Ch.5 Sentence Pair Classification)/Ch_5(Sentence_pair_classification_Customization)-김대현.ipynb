{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. ë°ì´í„° ì‚¬ìš©í•˜ê¸°\n",
        "-  ì¸ê³µì§€ëŠ¥ ê¸°ì—… â€˜ì—…ìŠ¤í…Œì´ì§€â€™ì—ì„œ ê³µê°œí•œ NLI ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ê³  ìˆëŠ”ë°ìš”.\n",
        "- ë‚˜ë§Œì˜ ë¬¸ì„œ ë¶„ë¥˜ ëª¨ë¸ êµ¬ì¶•ì„ ìœ„í•œ ì²«ê±¸ìŒì€ ë‚´ê°€ ê°€ì§„ ë°ì´í„°ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì¼ ê²ë‹ˆë‹¤.\n",
        "- ì´ë¥¼ ìœ„í•´ì„œëŠ” ë§ë­‰ì¹˜ë¥¼ ì½ì–´ë“¤ì´ëŠ” ì½”ë“œì— ëŒ€í•œ ì´í•´ê°€ ì„ í–‰ë˜ì–´ì•¼ í• í…ë°ìš”.\n",
        "- ì—¬ê¸°ì„œ KorNLI ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ì½ê³  ì „ì²˜ë¦¬í•˜ê³  ìˆëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤"
      ],
      "metadata": {
        "id": "cdCuN2bAAZh1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D69BUePKAPGK"
      },
      "outputs": [],
      "source": [
        "# ratsnlp ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
        "!pip install ratsnlp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# êµ¬ê¸€ë“œë¼ì´ë¸Œì™€ ì—°ê²°\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "UFhpF1z6AoiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì¸í¼ëŸ°ìŠ¤, ëª¨ë¸ í™˜ê²½ ì„¤ì •\n",
        "import torch\n",
        "from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n",
        "args = ClassificationTrainArguments(\n",
        "    pretrained_model_name=\"beomi/kcbert-base\", # í”„ë¦¬íŠ¸ë ˆì¸ ë§ˆì¹œ ì–¸ì–´ëª¨ë¸ì˜ ì´ë¦„\n",
        "    downstream_corpus_name=\"nsmc\", # ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ë°ì´í„°ì˜ ì´ë¦„.\n",
        "    downstream_model_dir=\"/gdrive/My Drive/nlpbook/checkpoint-doccls\", # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì˜ ì²´í¬í¬ì¸íŠ¸ê°€ ì €ì¥ë  ìœ„ì¹˜\n",
        "    batch_size=32 if torch.cuda.is_available() else 4, # batch_size\n",
        "    learning_rate=5e-5, # learning rate\n",
        "    max_seq_length=128, # í† í° ê¸°ì¤€ ì…ë ¥ ë¬¸ì¥ ìµœëŒ€ ê¸¸ì´\n",
        "    epochs=3, # í•™ìŠµ íšŸìˆ˜\n",
        "    tpu_cores=0 if torch.cuda.is_available() else 8, # TPU ì½”ì–´ ìˆ˜\n",
        "    seed=7, # ëœë¤ ì‹œë“œ\n",
        ")"
      ],
      "metadata": {
        "id": "ibqYOVmOApiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë§ë­‰ì¹˜ ë‹¤ìš´ë¡œë“œ\n",
        "from Korpora import Korpora\n",
        "Korpora.fetch(\n",
        "    corpus_name=args.downstream_corpus_name,\n",
        "    root_dir=args.downstream_corpus_root_dir,\n",
        "    force_download=True,\n",
        ")"
      ],
      "metadata": {
        "id": "A5R7OmENAqkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        "    do_lower_case=False,\n",
        ")"
      ],
      "metadata": {
        "id": "0JH7RoSHAsOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë“œ1. KORNLI ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬\n",
        "# ë°ì´í„° ë¡œë”©\n",
        "from ratsnlp.nlpbook.paircls import KlueNLICorpus\n",
        "corpus = KlueNLICorpus()\n",
        "\n",
        "# ë°ì´í„° ì „ì²˜ë¦¬\n",
        "from ratsnlp.nlpbook.classification import ClassificationDataset\n",
        "train_dataset = ClassificationDataset(\n",
        "\targs=args,\n",
        "\tcorpus=corpus,\n",
        "\ttokenizer=tokenizer,\n",
        "\tmode=\"train\",\n",
        ")"
      ],
      "metadata": {
        "id": "A9tTN8_UAsnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ìœ„ì˜ ì½”ë“œì—ì„œ ì„ ì–¸í•œ KlueNLICorpus í´ë˜ìŠ¤ëŠ”NLI ë°ì´í„°ë¥¼ íŒŒì´ì¬ ë¬¸ìì—´(string) ìë£Œí˜•ìœ¼ë¡œ ì½ì–´ë“¤ì´ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
        "- KlueNLICorpus í´ë˜ìŠ¤ì˜ êµ¬ì²´ì  ë‚´ìš©ì€ ì½”ë“œ2ì™€ ê°™ìŠµë‹ˆë‹¤.\n",
        "- ì´ í´ë˜ìŠ¤ì˜ get_examples ë©”ì†Œë“œëŠ” NLI ë°ì´í„°ë¥¼ ì½ì–´ë“¤ì´ê³  get_labelsëŠ” NLI ë°ì´í„°ì˜ ëª¨ë“  ë ˆì´ë¸” ì¢…ë¥˜(entailment, contradiction, neutral)ë¥¼ ë°˜í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.\n",
        "- ClassificationDatasetëŠ” KlueNLICorpus í´ë˜ìŠ¤ì˜ get_examples ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë§ë­‰ì¹˜ë¥¼ ì½ì–´ë“¤ì´ëŠ”ë°ìš”.\n",
        "- ë”°ë¼ì„œ KlueNLICorpus í´ë˜ìŠ¤ì˜ get_examplesë¥¼ ìì‹ ì´ ê°€ì§„ ë§ë­‰ì¹˜ì— ë§ê²Œ ê³ ì¹˜ë©´ ëª¨ë¸ì„ ì‚¬ìš©ìê°€ êµ¬ì„±í• ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ZML2Or81AzS6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë“œ2. KLUENLICORPUS í´ë˜ìŠ¤\n",
        "import os, csv\n",
        "from ratsnlp.nlpbook.classification.corpus import ClassificationExample\n",
        "\n",
        "class KlueNLICorpus:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def _create_examples(self, data_path):\n",
        "        examples = []\n",
        "        data = json.load(open(data_path, \"r\"))\n",
        "        for el in data:\n",
        "            example = ClassificationExample(\n",
        "                text_a=el[\"premise\"],\n",
        "                text_b=el[\"hypothesis\"],\n",
        "                label=el[\"gold_label\"],\n",
        "            )\n",
        "            examples.append(example)\n",
        "        return examples\n",
        "\n",
        "    def get_examples(self, data_path, mode):\n",
        "        if mode == \"train\":\n",
        "            data_fpath = os.path.join(data_path, \"klue_nli_train.json\")\n",
        "        else:\n",
        "            data_fpath = os.path.join(data_path, \"klue_nli_dev.json\")\n",
        "        logger.info(f\"loading {mode} data... LOOKING AT {data_fpath}\")\n",
        "        examples = self._create_examples(data_fpath)\n",
        "        return examples\n",
        "\n",
        "    def get_labels(self):\n",
        "        return [\"entailment\", \"contradiction\", \"neutral\"]\n",
        "\n",
        "    @property\n",
        "    def num_labels(self):\n",
        "        return len(self.get_labels())"
      ],
      "metadata": {
        "id": "bEIfoPVaBAX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì œ ì»¤ìŠ¤í…€ ë§ë­‰ì¹˜ í´ë˜ìŠ¤ë¥¼ ë§Œë“¤ì–´ ë´…ì‹œë‹¤. ì˜ˆì»¨ëŒ€ ìš°ë¦¬ê°€ ê°€ì§„ í•™ìŠµë°ì´í„°ì˜ íŒŒì¼ ì´ë¦„ì´ train.txtì´ê³ ê° ë ˆì½”ë“œê°€ ë‹¤ìŒê³¼ ê°™ì´ ì§„ìˆ , ê°€ì„¤ ë¬¸ì¥, ê·¸ë¦¬ê³  ì§„ìˆ ê³¼ ê°€ì„¤ ì‚¬ì´ì˜ ê´€ê³„(ë ˆì´ë¸”)ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤.\n",
        "\n",
        "\n",
        "```\n",
        "ì˜¤ëŠ˜ ê³µì›ì—ì„œ ì¹œêµ¬ë¥¼ ë§Œë‚¬ë‹¤,ì˜¤ëŠ˜ ê³µì›ì— ê°”ë‹¤,í•¨ì˜\n",
        "ì˜¤ëŠ˜ ê³µì›ì—ì„œ ì¹œêµ¬ë¥¼ ë§Œë‚¬ë‹¤,ì˜¤ëŠ˜ ê³µì›ì— ê°€ì§€ ì•Šì•˜ë‹¤,ëª¨ìˆœ\n",
        "ì˜¤ëŠ˜ ê³µì›ì—ì„œ ì¹œêµ¬ë¥¼ ë§Œë‚¬ë‹¤,ì˜¤ëŠ˜ ë°¥ì„ ë¨¹ì—ˆë‹¤,ì¤‘ë¦½\n",
        "...\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "4K4xg9UbBGTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì´ ë§ë­‰ì¹˜ë¥¼ ì½ì–´ë“¤ì¼ ìˆ˜ ìˆë„ë¡ í´ë˜ìŠ¤ë¥¼ ìƒˆë¡œ ì •ì˜í•œ ê²ƒì€ ì½”ë“œ3ì…ë‹ˆë‹¤.\n",
        "-  CustomNLICorpus í´ë˜ìŠ¤ì˜ get_examplesê°€ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¼ì¸(line) ë‹¨ìœ„ë¡œ ì½ì–´ë“¤ì¸ ë’¤ ì‰¼í‘œ(,)ë¡œ ì§„ìˆ , ê°€ì„¤, ë ˆì´ë¸”ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤.\n",
        "- ì´í›„ ì§„ìˆ ì€ ClassificationExampleì˜ text_a ì—, ê°€ì„¤ì€ text_bì—, ë‘˜ ì‚¬ì´ì˜ ê´€ê³„ëŠ” labelì— ì €ì¥í•´ ë‘¡ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "wNDa_UkyBOHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë“œ3. ì»¤ìŠ¤í…€ ë§ë­‰ì¹˜ í´ë˜ìŠ¤\n",
        "import os\n",
        "from ratsnlp.nlpbook.classification import ClassificationExample\n",
        "\n",
        "class CustomNLICorpus:\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def get_examples(self, data_root_path, mode):\n",
        "        data_fpath = os.path.join(data_root_path, f\"{mode}.txt\")\n",
        "        lines = open(data_fpath, \"r\", encoding=\"utf-8\").readlines()\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            if i == 0:\n",
        "                continue\n",
        "            text_a, text_b, label = line\n",
        "            examples.append(ClassificationExample(text_a=text_a, text_b=text_b, label=label))\n",
        "        return examples\n",
        "\n",
        "    def get_labels(self):\n",
        "        return [\"í•¨ì˜\", \"ëª¨ìˆœ\", \"ì¤‘ë¦½\"]\n",
        "\n",
        "    @property\n",
        "    def num_labels(self):\n",
        "        return len(self.get_labels())"
      ],
      "metadata": {
        "id": "RK5MhXxtBWXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- í•œí¸ CustomNLICorpus í´ë˜ìŠ¤ì˜ get_labels ë©”ì†Œë“œëŠ” ë¶„ë¥˜ ëŒ€ìƒ ë ˆì´ë¸”ì˜ ì¢…ë¥˜ë¥¼ ë¦¬í„´í•˜ëŠ” ì—­í• ì„ í•˜ëŠ” í•¨ìˆ˜ì¸ë°ìš”.\n",
        "\n",
        "- ìœ„ì˜ ì½”ë“œì—ì„œëŠ” ì´ë¥¼ í•˜ë“œ ì½”ë”©ìœ¼ë¡œ [â€œí•¨ì˜â€, â€œëª¨ìˆœâ€, â€œì¤‘ë¦½â€]ë¼ê³  ëª…ì‹œí–ˆìŠµë‹ˆë‹¤ë§Œ, ë§ë­‰ì¹˜ë¥¼ ì½ì–´ë“¤ì¸ ë’¤ í•´ë‹¹ ë§ë­‰ì¹˜ì˜ ë ˆì´ë¸”ì„ ì „ìˆ˜ ì¡°ì‚¬í•œ ë’¤ ìœ ë‹ˆí¬í•œ ë ˆì´ë¸”ë“¤ë§Œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ë¦¬í„´í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•´ë„ ìƒê´€ ì—†ìŠµë‹ˆë‹¤.\n",
        "\n",
        "- ì½”ë“œ4ëŠ” ì½”ë“œ3ì—ì„œ ì •ì˜í•œ ì»¤ìŠ¤í…€ ë°ì´í„°ì— ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤. ë§Œì¼ í‰ê°€ìš© ë°ì´í„°ì…‹ìœ¼ë¡œ valid.txtë¥¼ ê°€ì§€ê³  ìˆë‹¤ë©´ ì½”ë“œ4ì—ì„œ mode=\"valid\" ì¸ìë¥¼ ì£¼ì–´ì„œ val_datasetë„ ì„ ì–¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "DsZoarkPBZWY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# í† í¬ë‚˜ì´ì € ì¤€ë¹„\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        "    do_lower_case=False,\n",
        ")"
      ],
      "metadata": {
        "id": "XbT0TOGLByx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë“œ4. ì»¤ìŠ¤í…€ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬\n",
        "from ratsnlp.nlpbook.classification import ClassificationDataset\n",
        "\n",
        "corpus = CustomNLICorpus()\n",
        "train_dataset = ClassificationDataset(\n",
        "\targs=args,\n",
        "\tcorpus=corpus,\n",
        "\ttokenizer=tokenier,\n",
        "\tmode=\"train\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "y0F1oEmdBp1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. í”¼ì²˜ êµ¬ì¶• ë°©ì‹ ì´í•´í•˜ê¸°\n",
        "- ClassificationDatasetì€ íŒŒì´í† ì¹˜ì˜ ë°ì´í„°ì…‹(Dataset) í´ë˜ìŠ¤ ì—­í• ì„ í•˜ëŠ” í´ë˜ìŠ¤ì…ë‹ˆë‹¤.\n",
        "- ëª¨ë¸ì´ í•™ìŠµí•  ë°ì´í„°ë¥¼ í’ˆê³  ìˆëŠ” ì¼ì¢…ì˜ ìë£Œ ì°½ê³ ë¼ê³  ì´í•´í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
        "- ë§Œì•½ì— ì´ë²ˆ í•™ìŠµì— ğ‘– ë²ˆì§¸ ë¬¸ì„œ-ë ˆì´ë¸”ì´ í•„ìš”í•˜ë‹¤ê³  í•˜ë©´ ìë£Œ ì°½ê³ ì—ì„œ ğ‘– ë²ˆì§¸ ë°ì´í„°ë¥¼ êº¼ë‚´ ì£¼ëŠ” ê¸°ëŠ¥ì´ í•µì‹¬ ì—­í• ì…ë‹ˆë‹¤.\n",
        "- ClassificationDatasetì€ 4ì¥ ë¬¸ì„œ ë¶„ë¥˜ íƒœìŠ¤í¬, ê·¸ë¦¬ê³  5ì¥ ë¬¸ì¥ ìŒ ë¶„ë¥˜ íƒœìŠ¤í¬ ëª¨ë‘ ìˆ˜í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "p-sg66Y-B0GZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ì½”ë“œ5ë¥¼ ì½”ë“œ4ì™€ ì—°ê´€ì§€ì–´ ì „ì²´ ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì´ ì–´ë–»ê²Œ ì´ë¤„ì§€ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.\n",
        "- ì½”ë“œ4ì—ì„œ CustomNLICorpus ClassificationDataset í´ë˜ìŠ¤ì˜ corpusë¡œ ë„£ì—ˆìŠµë‹ˆë‹¤.\n",
        "- ë”°ë¼ì„œ ClassificationDataset í´ë˜ìŠ¤ëŠ” CustomNLICorpus get_examples ë©”ì†Œë“œë¥¼ í˜¸ì¶œí•´ ì§„ìˆ , ê°€ì„¤, ë ˆì´ë¸”ì„ ClassificationExample í˜•íƒœë¡œ ì½ì–´ë“¤ì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "tKRfoAy7B09E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë“œ5. CLASSIFICATIONDATASET í´ë˜ìŠ¤ - ì‹¤í–‰ ì•ˆë©ë‹ˆë‹¤. ëª¨ë¸ì˜ ì‚¬ìš©ë˜ëŠ” í´ë˜ìŠ¤ê°€ ì¼ë¶€ ì—†ìŒ.\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from transformers import PreTrainedTokenizer\n",
        "from ratsnlp.nlpbook.classification.arguments import ClassificationTrainArguments\n",
        "from ratsnlp.nlpbook.classification import _convert_examples_to_classification_features\n",
        "\n",
        "class ClassificationDataset(Dataset):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            args: ClassificationTrainArguments,\n",
        "            tokenizer: PreTrainedTokenizer,\n",
        "            corpus,\n",
        "            mode: Optional[str] = \"train\",\n",
        "            convert_examples_to_features_fn=_convert_examples_to_classification_features,\n",
        "    ):\n",
        "        ...\n",
        "            self.corpus = corpus\n",
        "        ...\n",
        "                examples = self.corpus.get_examples(corpus_path, mode)\n",
        "                self.features = convert_examples_to_features_fn(\n",
        "                    examples,\n",
        "                    tokenizer,\n",
        "                    args,\n",
        "                    label_list=self.corpus.get_labels(),\n",
        "                )\n",
        "        ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self.features[i]\n",
        "\n",
        "    def get_labels(self):\n",
        "        return self.corpus.get_labels()"
      ],
      "metadata": {
        "id": "ZgrcUItOCHIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ClassificationDataset í´ë˜ìŠ¤ëŠ” ì´í›„ _convert_examples_to_classification_features í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•´ ì•ì„œ ì½ì–´ë“¤ì¸ exampleì„ featureë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
        "- convert_examples_to_classification_featuresê°€ í•˜ëŠ” ì—­í• ì€ ë¬¸ì„œ ìŒ(ì§„ìˆ , ê°€ì„¤)-ë ˆì´ë¸”ì„ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ê°€ê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n",
        "- ë‹¤ì‹œ ë§í•´ ë¬¸ì¥ì„ í† í°í™”í•˜ê³  ì´ë¥¼ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ëŠ” í•œí¸, ë ˆì´ë¸” ì—­ì‹œ ì •ìˆ˜(integer)ë¡œ ë°”ê¿”ì£¼ëŠ” ê¸°ëŠ¥ì„ í•©ë‹ˆë‹¤.\n",
        "- ì´ì™€ ê´€ë ¨í•´ ìì„¸í•œ ë‚´ìš©ì€ 4-2ì¥ Trainingì„ ì°¸ê³ í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "- í•œí¸ ClassificationDataset í´ë˜ìŠ¤ì˜ convert_examples_to_features_fn ì¸ìë¡œ ê¸°ë³¸ê°’ì¸ _convert_examples_to_classification_features ë§ê³  ë‹¤ë¥¸ í•¨ìˆ˜ë¥¼ ë„£ì–´ì¤„ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
        "- ì´ ê²½ìš° í”¼ì²˜ êµ¬ì¶•ì€ í•´ë‹¹ í•¨ìˆ˜ë¡œ ì§„í–‰í•˜ê²Œ ë©ë‹ˆë‹¤. ë‹¨, í•´ë‹¹ í•¨ìˆ˜ì˜ ê²°ê³¼ë¬¼ì€ List[ClassificationFeatures] í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.\n",
        "- ClassificationFeaturesì˜ êµ¬ì„± ìš”ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "```\n",
        "input_ids: List[int]\n",
        "attention_mask: List[int]\n",
        "token_type_ids: List[int]\n",
        "label: int\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "3VGgFc75CRuy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°\n",
        "- ìš°ë¦¬ ì±… ë¬¸ì„œ ë¶„ë¥˜ íŠœí† ë¦¬ì–¼ì—ì„œëŠ” ì´ì¤€ë²” ë‹˜ì´ ê³µê°œí•œ kcbertë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\n",
        "- í—ˆê¹…í˜ì´ìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ë“±ë¡ëœ ëª¨ë¸ì´ë¼ë©´ ë³„ë‹¤ë¥¸ ì½”ë“œ ìˆ˜ì • ì—†ì´ ë‹¤ë¥¸ ì–¸ì–´ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "- ì˜ˆì»¨ëŒ€ bert-base-uncased ëª¨ë¸ì€ êµ¬ê¸€ì´ ê³µê°œí•œ ë‹¤êµ­ì–´ BERT ëª¨ë¸ì¸ë°ìš”.\n",
        "- pretrained_model_nameì— í•´ë‹¹ ëª¨ë¸ëª…ì„ ì…ë ¥í•˜ë©´ ì´ ëª¨ë¸ì„ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ZD9H4g4HCfgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë“œ6. ë‹¤ë¥¸ ëª¨ë¸ ì‚¬ìš©í•˜ê¸° - argumentê°€ ë‹¤ ì—†ì–´ì„œ ì‘ë™ ì•ˆí•©ë‹ˆë‹¤. ì½”ë“œ\n",
        "from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n",
        "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification\n",
        "args = ClassificationTrainArguments(\n",
        "    pretrained_model_name=\"bert-base-uncased\", # pretrained_model_nameì— í•´ë‹¹ ëª¨ë¸ëª…ì„ ì…ë ¥í•˜ë©´ ì´ ëª¨ë¸ì„ ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\n",
        "    ...\n",
        ")\n",
        "tokenizer = BertTokenizer.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        "    do_lower_case=False,\n",
        ")\n",
        "pretrained_model_config = BertConfig.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        ")\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    args.pretrained_model_name,\n",
        "    config=pretrained_model_config,\n",
        ")"
      ],
      "metadata": {
        "id": "iRsUfcZ3ClnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "í—ˆê¹…í˜ì´ìŠ¤ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ì€ ë‹¤ìŒ ë§í¬ë¥¼ ì°¸ê³ í•˜ì‹œë©´ ë©ë‹ˆë‹¤.\n",
        "\n",
        "https://huggingface.co/models"
      ],
      "metadata": {
        "id": "MkGMEc_wCmKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. íƒœìŠ¤í¬ ì´í•´í•˜ê¸°\n",
        "- ì—¬ê¸°ì„œëŠ” íŒŒì´í† ì¹˜ ë¼ì´íŠ¸ë‹(pytorch lightning)ì˜ ë¼ì´íŠ¸ë‹ëª¨ë“ˆ(LightningModule) í´ë˜ìŠ¤ë¥¼ ìƒì† ë°›ì•„ íƒœìŠ¤í¬(task)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "- ì´ íƒœìŠ¤í¬ì—ëŠ” ëª¨ë¸(model)ê³¼ ì˜µí‹°ë§ˆì´ì €(optimizer), í•™ìŠµ ê³¼ì • ë“±ì´ ì •ì˜ë¼ ìˆìŠµë‹ˆë‹¤. ì´ì™€ ê´€ë ¨ëœ íŠœí† ë¦¬ì–¼ ì½”ë“œëŠ” ì•„ë˜ì— ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "MDJHC3fhCq68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë“œ7. ë¬¸ì„œ ë¶„ë¥˜ íƒœìŠ¤í¬ ì •ì˜\n",
        "from ratsnlp.nlpbook.classification import ClassificationTask\n",
        "task = ClassificationTask(model, args)"
      ],
      "metadata": {
        "id": "xZuY3-EaCsOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ClassificationTaskëŠ” ëŒ€ë¶€ë¶„ì˜ ë¬¸ì„œ ë¶„ë¥˜ íƒœìŠ¤í¬ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ ì¼ë°˜í™”ë˜ì–´ ìˆì–´ ë§ë­‰ì¹˜ ë“±ì´ ë°”ë€Œë”ë¼ë„ ì½”ë“œ ìˆ˜ì •ì„ ë³„ë„ë¡œ í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
        "- ë‹¤ë§Œ í•´ë‹¹ í´ë˜ìŠ¤ê°€ ì–´ë–¤ ì—­í• ì„ í•˜ê³  ìˆëŠ”ì§€ ì¶”ê°€ ì„¤ëª…ì´ í•„ìš”í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.\n",
        "- ì•„ë˜ì˜ ì½”ë“œëŠ” ìœ„ì˜ ì½”ë“œê°€ ì´ ì‚¬ìš©í•˜ëŠ” ClassificationTask í´ë˜ìŠ¤ë¥¼ ìì„¸í•˜ê²Œ ë‚˜íƒ€ë‚¸ ê²ƒì…ë‹ˆë‹¤. - ì•„ë˜ ì½”ë“œì˜ íƒœìŠ¤í¬ í´ë˜ìŠ¤ì˜ ì£¼ìš” ë©”ì†Œë“œì— ê´€í•œ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "\n",
        "```\n",
        "configure_optimizers : ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ì˜µí‹°ë§ˆì´ì €(optimizer)ì™€ ëŸ¬ë‹ë ˆì´íŠ¸ ìŠ¤ì¼€ì¤„ëŸ¬(learning rate scheduler)ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë‹¤ë¥¸ ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ì´ ë©”ì†Œë“œì˜ ë‚´ìš©ì„ ê³ ì¹˜ë©´ ë©ë‹ˆë‹¤.\n",
        "training_step : í•™ìŠµ(train) ê³¼ì •ì—ì„œ í•œ ê°œì˜ ë¯¸ë‹ˆë°°ì¹˜(inputs)ê°€ ì…ë ¥ëì„ ë•Œ ì†ì‹¤(loss)ì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "validation_step : í‰ê°€(validation) ê³¼ì •ì—ì„œ í•œ ê°œì˜ ë¯¸ë‹ˆë°°ì¹˜(inputs)ê°€ ì…ë ¥ëì„ ë•Œ ì†ì‹¤(loss)ì„ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "fgFpmd90Ctm_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë“œ8. ë¬¸ì„œ ë¶„ë¥˜ íƒœìŠ¤í¬ í´ë˜ìŠ¤\n",
        "from transformers import PreTrainedModel\n",
        "from transformers.optimization import AdamW\n",
        "from ratsnlp.nlpbook.metrics import accuracy\n",
        "from pytorch_lightning import LightningModule\n",
        "from torch.optim.lr_scheduler import ExponentialLR, CosineAnnealingWarmRestarts\n",
        "from ratsnlp.nlpbook.classification.arguments import ClassificationTrainArguments\n",
        "\n",
        "class ClassificationTask(LightningModule):\n",
        "    def __init__(self, model: PreTrainedModel, args: ClassificationTrainArguments):\n",
        "        \"\"\"\n",
        "        ë¬¸ì„œ ë¶„ë¥˜ íƒœìŠ¤í¬ í´ë˜ìŠ¤ì˜ ìƒì„±ìì…ë‹ˆë‹¤.\n",
        "\n",
        "        Args:\n",
        "            model (PreTrainedModel): ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë˜ëŠ” ì‚¬ìš©ì ì •ì˜ ëª¨ë¸.\n",
        "            args (ClassificationTrainArguments): í•™ìŠµ ì„¤ì •ì„ ë‹´ì€ Argument ê°ì²´.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        \"\"\"\n",
        "        ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì„¤ì •í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.\n",
        "\n",
        "        Returns:\n",
        "            dict: ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ë‹´ì€ ë”•ì…”ë„ˆë¦¬.\n",
        "        \"\"\"\n",
        "        if self.args.optimizer == 'AdamW':\n",
        "            optimizer = AdamW(self.parameters(), lr=self.args.learning_rate)\n",
        "        else:\n",
        "            raise NotImplementedError('Only AdamW is Supported!')\n",
        "\n",
        "        if self.args.lr_scheduler == 'cos':\n",
        "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
        "        elif self.args.lr_scheduler == 'exp':\n",
        "            scheduler = ExponentialLR(optimizer, gamma=0.5)\n",
        "        else:\n",
        "            raise NotImplementedError('Only cos and exp lr scheduler is Supported!')\n",
        "\n",
        "        return {'optimizer': optimizer, 'scheduler': scheduler}\n",
        "\n",
        "    def training_step(self, inputs, batch_idx):\n",
        "        \"\"\"\n",
        "        í›ˆë ¨ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.\n",
        "\n",
        "        Args:\n",
        "            inputs (dict): ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ë°ì´í„° ë°°ì¹˜.\n",
        "            batch_idx (int): í˜„ì¬ ë°°ì¹˜ì˜ ì¸ë±ìŠ¤.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: ì†ì‹¤ ê°’.\n",
        "        \"\"\"\n",
        "        # outputs: SequenceClassifierOutput\n",
        "        outputs = self.model(**inputs)\n",
        "        preds = outputs.logits.argmax(dim=-1)\n",
        "        labels = inputs[\"labels\"]\n",
        "        acc = accuracy(preds, labels)\n",
        "        self.log(\"loss\", outputs.loss, prog_bar=False, logger=True, on_step=True, on_epoch=False)\n",
        "        self.log(\"acc\", acc, prog_bar=True, logger=True, on_step=True, on_epoch=False)\n",
        "        return outputs.loss\n",
        "\n",
        "    def validation_step(self, inputs, batch_idx):\n",
        "        \"\"\"\n",
        "        ê²€ì¦ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë©”ì„œë“œì…ë‹ˆë‹¤.\n",
        "\n",
        "        Args:\n",
        "            inputs (dict): ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë  ê²€ì¦ ë°ì´í„° ë°°ì¹˜.\n",
        "            batch_idx (int): í˜„ì¬ ë°°ì¹˜ì˜ ì¸ë±ìŠ¤.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: ì†ì‹¤ ê°’.\n",
        "        \"\"\"\n",
        "        # outputs: SequenceClassifierOutput\n",
        "        outputs = self.model(**inputs)\n",
        "        preds = outputs.logits.argmax(dim=-1)\n",
        "        labels = inputs[\"labels\"]\n",
        "        acc = accuracy(preds, labels)\n",
        "        self.log(\"val_loss\", outputs.loss, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
        "        self.log(\"val_acc\", acc, prog_bar=True, logger=True, on_step=False, on_epoch=True)\n",
        "        return outputs.loss\n"
      ],
      "metadata": {
        "id": "fpVd9tfgCziM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ìœ„ì˜ ì½”ë“œì˜ training_step, validation_step ë©”ì†Œë“œì—ì„  ë¯¸ë‹ˆ ë°°ì¹˜(input)ë¥¼ ëª¨ë¸ì— ë„£ì–´ ì†ì‹¤(loss), ë¡œì§“(logit) ë“±ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "- ëª¨ë¸ì˜ ìµœì¢… ì¶œë ¥ì€ â€˜ì…ë ¥ ë¬¸ì¥ ìŒì´ íŠ¹ì • ë²”ì£¼(ì°¸, ê±°ì§“, ì¤‘ë¦½)ì¼ í™•ë¥ â€™ì¸ë°ìš”. ë¡œì§“ì€ ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì·¨í•˜ê¸° ì§ì „ì˜ ë²¡í„°ì…ë‹ˆë‹¤.\n",
        "\n",
        "- ë¡œì§“(outputs.logits)ì— argmaxë¥¼ ì·¨í•´ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë¬¸ì„œ ë²”ì£¼ë¥¼ ê°€ë ¤ë‚´ê³  ì´ë¡œë¶€í„° ì •í™•ë„(accuracy)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "- ë¡œì§“ìœ¼ë¡œ ì˜ˆì¸¡ ë²”ì£¼(preds)ë¥¼ ë§Œë“œëŠ” ì´ìœ ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì·¨í•œë‹¤ê³  ëŒ€ì†Œ ê´€ê³„ê°€ ë°”ë€ŒëŠ” ê²ƒì€ ì•„ë‹ˆë‹ˆ, ë¡œì§“ìœ¼ë¡œ argmaxë¥¼ í•˜ë”ë¼ë„ ì˜ˆì¸¡ ë²”ì£¼ê°€ ë‹¬ë¼ì§€ì§„ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "- ì´í›„ ì†ì‹¤, ì •í™•ë„ ë“±ì˜ ì •ë³´ë¥¼ ë¡œê·¸ì— ë‚¨ê¸´ ë’¤ ë©”ì†Œë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.\n",
        "\n",
        "- ìœ„ì˜ ì½”ë“œì˜ training_step, validation_step ë©”ì†Œë“œëŠ” self.modelì„ í˜¸ì¶œ(call)í•´ ì†ì‹¤ê³¼ ë¡œì§“ì„ ê³„ì‚°í•˜ëŠ”ë°ìš”.\n",
        "- self.modelì€ ì•„ë˜ ì½”íŠ¸ì˜ BertForSequenceClassification í´ë˜ìŠ¤ë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤.\n",
        "\n",
        "- ì—¬ê¸°ì„œëŠ” í—ˆê¹…í˜ì´ìŠ¤ì˜ íŠ¸ëœìŠ¤í¬ë¨¸(transformers) ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì œê³µí•˜ëŠ” í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê·¸ í•µì‹¬ë§Œ ë°œì·Œí•œ ì½”ë“œëŠ” ì•„ë˜ ì½”ë“œì™€ ê°™ìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "9nfj9n42DMmV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ì½”ë“œ9 BERTFORSEQUENCECLASSIFICATION\n",
        "class BertForSequenceClassification(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        # BERT ê¸°ë°˜ ì‹œí€€ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ ì´ˆê¸°í™”\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "\n",
        "        # BERT ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "        self.bert = BertModel(config)\n",
        "\n",
        "        # ë“œë¡­ì•„ì›ƒ ë ˆì´ì–´\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # ì„ í˜• ë¶„ë¥˜ ë ˆì´ì–´\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        # BERT ëª¨ë¸ì˜ forward ë©”ì†Œë“œ êµ¬í˜„\n",
        "\n",
        "        # BERT ëª¨ë¸ì— ì…ë ¥ê°’ ì „ë‹¬í•˜ì—¬ ì¶œë ¥ê°’ íšë“\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # BERT ëª¨ë¸ì˜ ì¶œë ¥ ì¤‘ pooled_output ì‚¬ìš©\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        # ë“œë¡­ì•„ì›ƒ ì ìš©\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "\n",
        "        # ì„ í˜• ë¶„ë¥˜ ë ˆì´ì–´ì— ì ìš©í•˜ì—¬ ë¡œì§“ íšë“\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        # ì†ì‹¤ ê³„ì‚°\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.num_labels == 1:\n",
        "                # 1ê°œì˜ ë ˆì´ë¸”ì´ ìˆëŠ” ê²½ìš° (íšŒê·€)\n",
        "                loss_fct = MSELoss()\n",
        "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
        "            else:\n",
        "                # ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì˜ ê²½ìš°\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        # ê²°ê³¼ ë°˜í™˜\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "6T6DqTcWDWRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ìœ„ì˜ ì½”ë“œì˜ self.bertëŠ” BERT ëª¨ë¸ì„ ê°€ë¦¬í‚µë‹ˆë‹¤.\n",
        "- ë¹ˆì¹¸ ë§ì¶”ê¸°, ì¦‰ ë§ˆìŠ¤í¬ ì–¸ì–´ëª¨ë¸(Masked Language Model)ë¡œ í”„ë¦¬íŠ¸ë ˆì¸ì„ ì´ë¯¸ ì™„ë£Œí•œ ëª¨ë¸ì…ë‹ˆë‹¤.\n",
        "- self.dropoutì™€ self.classifierëŠ” 4-1ì¥ì—ì„œ ì†Œê°œí•œ ë¬¸ì„œ ë¶„ë¥˜ íƒœìŠ¤í¬ ëª¨ë“ˆì´ ë˜ê² ìŠµë‹ˆë‹¤.\n",
        "- NLI ë°ì´í„°ì— ëŒ€í•´ ì§„ìˆ , ê°€ì„¤ ì‚¬ì´ì˜ ê´€ê³„(ì°¸, ê±°ì§“, ì¤‘ë¦½)ë¥¼ ìµœëŒ€í•œ ì˜ ë§ì¶”ëŠ” ë°©í–¥ìœ¼ë¡œ self.bert, self.classifierê°€ í•™ìŠµë©ë‹ˆë‹¤.\n",
        "\n",
        "- í•œí¸ ì½”ë“œ8ì˜ training_step, validation_step ë©”ì†Œë“œì—ì„œ self.modelì„ í˜¸ì¶œí•˜ë©´ BertForSequenceClassificationì˜ forward ë©”ì†Œë“œê°€ ì‹¤í–‰ë©ë‹ˆë‹¤.\n",
        "- ë‹¤ì‹œ ë§í•´ ì½”ë“œ8ì˜ training_step, validation_step ë©”ì†Œë“œëŠ” self.model ë©”ì†Œë“œì™€ ì§ì„ ì§€ì–´ êµ¬í˜„í•´ì•¼ í•œë‹¤ëŠ” ì´ì•¼ê¸°ì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "fpl0xArxDce5"
      }
    }
  ]
}